---
title: "GLharmonizeR - bringing together multiple sources of water quality data"
title-slide-attributes:
  data-background-image: ../../figsTables/logoNoWords.png
  data-background-size: contain
  data-background-opacity: "0.2"

author: 
  - name: Christian Coffman
    email: coffman.christian@epa.gov
    roles: 
      - Implementation
      - Design 
    affiliations: 
      - name: ORAU - ORD-CCTE-SCDCD-DMMB

  - name: Dr. Kelsey Vitense 
    roles: 
      - Mentor
      - Conceptualization
      - Design 
    affiliations: 
      - name: ORD-CCTE-SCDCD-DMMB
      

format:
  revealjs:
    embed-resources: true
    smaller: true
    scrollable: true
    theme: moon 
    logo: ../../figsTables/logo.png
    footer: "GLHarmonizeR"
    mermaid:
      theme: dark
execute: 
  cache: true

---

# My introduction

:::: {.columns}
::: {.column width="40%"}
```{r, myTravelMap}
#| echo: false
#| eval: true
#| file: images/travelMap.R
```

:::

::: {.column width="60%"}
- From / now working in Duluth
- Study physics/chemistry
- Started as EPA contractor last fall
- Projects:
  - hydrodynamic simulation model in Lake Ontario
  - Harmonizing water quality data across great lakes
:::
::::

```{mermaid myTimeline}
%%| file: images/myTimeline.mmd
```

## Acknowledgements

This work was supported by many folks across a couple of agencies

:::: {.columns}
::: {.column width="50%"}
This work was supported by many folks across EPA offices/regions, USGS, and NOAA
- ORD data/modeling team
  - James Pauer (GLTED)
  - Thomas Hollenhorst (GLTED)
  - Terry Brown (SCDCD)
  - Ryan Lepak (GLTED)
  - Aabir Banerji (GLTED)
  - Brandon Jarvis (CEMM)
  - Wilson Melendez (GDIT)
  - Jonathon Launspach (GDIT)
:::

::: {.column width="50%"}

- **Data owners**
- CSMI
  - Joel Hoffman (GLTED)
  - Anett Trebitz (GLTED)
  - Ralph Tingley (USGS)
- GLENDA
  - Elizabeth Hinchey (GLNPO)
  - Kenneth Klewin (GLNPO)
  - Eric Osantowski (GLNPO)
- NCCA
  - Hugh Sullivan (OW)
  - Sarah Lehmann (OW)
- NOAA
  - Steve Pothoven (NOAA/GLERL)
:::
::::
WHAT USE FOR PICTURE HERE?

# Project context
## Algal blooms can be harmful
:::: {.columns}
::: {.column width="50%"}
- [Harmful algal blooms (HABs) - toxic outgrowths of algae]((https://www.iwla.org/publications/outdoor-america/articles/outdoor-america-2020-issue-2/algal-blooms-the-causes-dangers-and-some-solutions))
  - microcystins damage liver, brain
  - more potent than cyanide
- [HAB frequency increased recent years](https://pubs.acs.org/doi/10.1021/acs.est.9b03726#:~:text=Widespread%20coastal%20eutrophication%20is%20known%20to%20increase%20the,to%20increased%20prevalence%20and%20earlier%20timing%20of%20HABs.)
:::

::: {.column width="50%"}
- Public burden
  - Beach closures
  - Make animals dangerously sick
:::
:::
![source: iwla.org](images\algalBloom.jpg)

## Modeled in Lake Erie using ML 
- ML models successfully predict HABs, or Chlorophyll-a (Chla), in Lake Erie
- [Measured nutrient concentrations necessary for reliable prediction](https://www.nature.com/articles/s43247-022-00510-w)

::: {#fig-elephants layout-ncol=1}

![](images/lakeErieR2.png)


Predictions in Lake Erie from [Gupta et al. 2023](https://www.sciencedirect.com/science/article/pii/S0048969723044042)
:::


## Main goals/questions 
- Use ML to predict Chl-a as proxy for HABS in Lake Michigan
- How accurately can we predict chl-a at specific location, time, and depth in the lake?
- What is the relative imporatance of chl-a predictors and a minimum covariate set for prediction?
- Apply similar ML approaches on Lake Michigan
- Use machine learning to predict chlorophyll-a as a proxy measure

::: {.callout-warning collapse="true"}
# No unified data source


ML models require lots of data for successful prediction
:::

## Sub-Goal: Make our data assemblage accessible to others 
- Assembling and harmonizing takes a lot of effort
- Other scientists want these data (Trend analyses, Correlation analyses, Broader applications)
- Inspired by similar R packages to harmonize limnological data
  - LAGOSUS: harmonize water quality data from WQP, NLA, and NEON- Focused on inland lakes
  - finsyncR: harmonize federal macroinvertebrate and fish data 


## Data Overview

| Source | Purpose |  Access |
| :----- | :------ |  :----- |
| Great Lakes National Program Office survey (GLNPO) | Track changes in water quality of Great Lakes open waters | GLENDA on EPA Central Data Exchange (CDX); Password protected API |
| National Coastal Condition Assessment survey (NCCA) | Assess condition of coastal waters | Freely available on NARS website |
| Cooperative Science and Monitoring Initiative survey (CSMI) | US/Canada-coordinated GL science/monitoring under GLWQA | Local Access databases and Excel spreadsheets |
| Great Lakes Environmental Research Laboratory survey (NOAA) | Annual survey for SE Lake Michigan |  By Request from NOAA  |


# Project design

:::: {.columns}
::: {.column width="40%"}
```{mermaid packageUse}
flowchart TB
  install[Install R package] --> query[Select data] --> download[Save locally]
```
:::

::: {.column width="40%"}
```{mermaid simpleSketch}
flowchart LR
  NOAA --> NormNOAA[Normalize] --> cleanNOAA[Clean] --> StanNOAA[Standardize] --> Join
  GLENDA --> NormGLENDA[Normalize] --> cleanGLENDA[Clean] --> StanGLENDA[Standardize] --> Join
  NCCA --> NormNCCA[Normalize] --> cleanNCCA[Clean] --> StanNCCA[Standardize] --> Join
  CSMI --> NormCSMI[Normalize] --> cleanCSMI[Clean] --> StanCSMI[Standardize] --> Join
```
:::
:::
- [Harmonize](https://www.nature.com/articles/s41597-024-02956-3) across data sources:
  - Normalize the format 
  - Standardize naming and units
  - Cleaning between normalize and standarize

## Benefits of our GLharmonizeR package
- Federal GL WQ data in one place, harmonized
- Incorporates institutional knowledge from conversations with data owners/managers
  - worked with GLNPO to add MDLs, additional lat/longs, identify additional QC issues, harmonization of methods over time
- Includes common water quality parameters across survey (aims for simplicity)
- R package/Shiny app to make it accessible to researchers


## WorkFlow diagram
```{mermaid idealDesign}
%%| file: ../../figsTables/design/functionDesign.mmd
```



## Workflow diagram {.scrollable}


- Update this figure
- Make the map how what makes sense 
  - If code doesn't match, then we should change the code
```{mermaid functionDiagram}
flowchart TB

  %%%%%%%%% Full graph
  nas --> columns 
  convertingUnits --> joinData 


  subgraph Repeated for each dataset


    %%%%%%%%% Data cleaning
    subgraph  Data Cleaning 
     data[(New Data)]
     format[\Reformat data\]
     Types[\ Enforce data types \n Format datetimes\]
     filtering[\ Preliminary filtering \n Sample types \n Medium \n QC flags \]
     nas[\Dealing with NA's \n Reporting imputed values  \]

     data --> format --> Types --> filtering --> nas  
    end

    %%%%%%%%% Harmonizing data
    subgraph Data Harmonizing

    %% source nodes
     analyteNamesUnits[("Unify analyte names \n and units \n Analytes3.xlsx \n 'Key'")]
     unitConversions[("Convert units\n Analytes3.xlsx \n 'UnitConversions'")]

    %% action nodes
     columns[\Standardize \n column names\]
     Naming[\"Harmonize \n Analyte names \n '<Data Source>_Map"\]
     targettingUnits[\"Setting target \n units \n 'Key' "\]
     convertingUnits[\"Convert to \n target units"\]


    %% edges
     analyteNamesUnits --variableid_lagos \n CharacteristicName \n source_parameter--> Naming 
     analyteNamesUnits --variableid_lagos \n variablename \n limit_low \n limit_high \n unitsabbreviation--> targettingUnits
     columns --ANALYTE--> Naming
     Naming --CharactersticName--> targettingUnits
     targettingUnits --units \n unitsabbreviation--> convertingUnits
     unitConversions --Code \n Target Unit \n Conversion Factor--> convertingUnits

     end

   end
   %%%%%%%%% Compiling data
   subgraph Compiling Data
    joinData[\Union of data\] 
    storeData[\ Store Data \n locally \]

    joinData -->storeData

  end
```


::: {.callout-tip}
## Lesson Learned

Spend time to design / diagram code structure
- Helps create bite-sized chunks
- Can design tests for test-based development
- Helps with consistent code
- Helps anticipate problems in meetings
:::

# Project Implementation


## Version control / collaboration

:::: {.columns}
::: {.column width="50%"}
- Version controlled using Git
- Feature branch workflow
- Github for: Collaboration, Issues, PR/Code Review, User docs wiki
- Technical / meeting notes in Microsoft One Note
- Difficult-to-access data stored on Github
- Developer environment controlled through Renv
:::

::: {.column width="50%"}
![](images/gitBranches.png)
:::
:::

::: {.callout-tip}
## Lesson Learned

Multiple tools OK for documentation note taking, especialy if it eases collaboration
:::


## Styling
:::: {.columns}
::: {.column width="59%"}
- Strict code styling
  - [tidyverse  style guide](https://style.tidyverse.org/) for consistency
  - [Google software engineering](https://abseil.io/resources/swe-book/html/toc.html) for design
- Standard R package structure
- Maintainabilty
![](images/niceComment.png)

::: {.callout-tip}
## Lesson Learned

adhering to code styles will make code reviews better/more enjoyable
- and reading through your old code less frustrating
:::

:::

::: {.column width="39%"}
![](images/sweGoogle.png)
:::
:::



## Standardization
- Standardization (renaming/unit conversions) updated through excel 
  - Amenable to future changes
  - Updating naming doesn't require programming expertise

![Mapping table](images/mappingTable.png)


## User Interface
![](images/shinyApp.png)

::: {.callout-tip}
## Lesson Learned

Good LSP saves time (Don't know Shiny, but had functioning front end prototype in few hours)
:::

- After the backend is finished
- shiny app drafted as a placeholder

## Quality Assurance (People power)
- Code Review
  - Mentor (Kelsey) and I

::: {.callout-tip}
## Lesson Learned

- Couldn't explain or remember how code work...  add a comment
- Someone asks about code... add a comment 
:::

- Beta  dataset to GLTED ecologists and modelers
- Beta package to ORD data scientists

## Quality Assurance (Automated)
- Unit testing
  - testthat package
  - Report by covr package
  - Janet Lee advised developing and organizing unit tests

- Github actions (under construction) - advised by Josh Powell 
  - Protect the main branch
  - Automate all Unit tests
  - Enforce code styling
  - Frees us humans up for code review


## Documentation

:::: {.columns}
::: {.column width="40%"}
- Product (under construction)
  - Installation
  - Usage
  - Details for Technical experts (environmental chemistry)
- Vignettes (under construction) on how to reformat the data for common modeling purposes
- Journal article (planned) 
  - Design
  - Development process/decisions
:::

::: {.column width="60%"}
![README contains contributor information. Styling, format, contributing, and discussions.](images/README.png)
:::
:::


## Implementing expert opinions
- Iterative/cyclic process
- Collaboration with CSMI, NCCA, NOAA, and GLNPO 
- Rigorous, consistent definition of WQ names - GLTED

```{mermaid devCycle}
%%| file: ../../figsTables/design/developmentCycle.mmd
```

::: {.callout-tip}
## Lesson Learned
Being overly communicative is better than underly communicative
:::

## Development timeline
```{mermaid devTimeline}
%%| file: ../../figsTables/design/v1RoadMap.mmd
```


# Future directions
- More user options (filtering)
- Add a "reccommended" cleaning function
- External tests (beta testing)
  - for data quality
  - Ease of package use
- Automated testing reports
  - Github actions
  - Unit test reporting
- Expand data sourcing
  - Incorporate state agency data from the water water quality portal
  - Incorporate tributary data
  - Expand to all the great lakes
  - Integrated statistical models "sharing" information across lakes
- Checking physical models





# Other Lessons learned

::: {.callout-tip}
## Lesson Learned

- If problem is intractable, don't know where to start. Just start somewhere it'll generally then give a clearer perspective on how  (documentation)
- good is better than perfect (Not always adhering to design aims can be good... in moderation)
- Workflow
  - Script
  - Write any checks I do as tests
  - Make it a function
- Creating one-off non-reproducible intermediates is OK to communicate to collaborators
- Good Coding / SWE practices
  - Almost always helps
  - Doesn't add much mental / time demand becomes part of your flow
:::


# Thank you 



# Technical details

:::: {.columns}

::: {.column width="40%"}
- Scattered
  - Local
  - Cloud
  - Private Databases
- Different formats
  - Dates
  - Coordinates
  - Feature naming
  - Information spread across variable numbers of columns
  - Long/wide
  - Headers, subtables
  - Metadata or not
  - Multifile / single file
:::

::: {.column width="60%"}
- Different file types
  - CSV
  - Excel
  - Microsoft Acess 2007
  - Hex
  - CNV
  - pdf
  - txt
  - rds
- Living
  - Need to reproducibly assemble and QC
:::

::::

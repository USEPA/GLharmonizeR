---
title: Imputing and pivoting
author: Christian Coffman
date: today
format:
  html:
    code-fold: true
    toc: true
    embed-resources: false
project:
  execute-dir: project
execute:
  warning: false
  error: false
---



```{r setup}
library(tidyverse)
df <- readRDS("fullData.Rds") %>% 
  filter(!grepl("no result", Orig_QAcomment, ignore.case = T)) %>%
  # some values are missing and don't have QA codes nor comments
  filter(
    !is.na(RESULT) | 
    !is.na(Orig_QAcode) |
    !is.na(Orig_QAcomment) |
    !is.na(Orig_QAdefinition) |
    !is.na(Unified_Flag) |
    !is.na(Unified_Comment)
  ) %>%
  # some missed qa actions not filtered
  filter(
    !grepl("invalid", Orig_QAcomment, ignore.case = T),
    !grepl("NRR", Orig_QAcomment, ignore.case = T),
  ) %>% 
  # sometimes unrelated QAcodes retain sample so removing that for Secchi
  #filter(
  #  !(is.na(RESULT) & CodeName == "Secchi" & grepl("T", Orig_QAcomment, ignore.case = T)),
  #) %>% 
  tidyr::drop_na(Latitude, sampleDateTime, CodeName)

df %>% 
  reframe(
    measureCount = sum(!is.na(RESULT)),
    missingCount = sum(is.na(RESULT)),
    .by = CodeName
  ) %>%
  arrange(desc(missingCount), desc(measureCount)) %>%
  gt::gt() %>%
  gt::tab_header("Pre-DL imputation")
```

# Detection limit informed imputation
```{r}
# [ ] Add in less than RL's here too
# [ ] Count the values that are <RL separately
# - if both mdl and rl igve priority to mdl
.dlImputation <- function(df, imputeMethod = NULL){
  # [ ] impute secchi depth as a separate function
  if (imputeMethod == "halfMDL"){
    imputeFunction = function(mdl) mdl/2
  } else if (imputeMethod == "uniform") {
    imputeFunction = function(mdl) runif(n = 1, min = 0, max=mdl)
  } else {
    # don't do anything
    imputeFunction = function(mdl) mdl
  }

  df <- df %>%
    dplyr::mutate(
      # resolve duplicates as average
      RESULT = mean(RESULT, na.rm =T), 
      # fill in missing data
      Study = toString(unique(Study)),
      Unified_Flag = toString(unique(Unified_Flag)),
      Unified_Comment = toString(unique(Unified_Comment)),
      Study = toString(unique(Study)),
      MDL = mean(MDL, na.rm =T), 
      # PQL = mean(PQL, na.rm =T),  dropped PQL might need to add back imputeMethodn
      RL = mean(RL, na.rm =T), 
      .by= c(Latitude, Longitude, sampleDateTime, CodeName, sampleDepth)) %>%
    # get yera for imputing
    dplyr::distinct() %>%
    dplyr::mutate(Year = lubridate::year(sampleDateTime)) %>%
    dplyr::mutate(
      # impute using DLs
      # https://19january2017snapshot.epa.gov/sites/production/files/2015-06/documents/whatthel.pdf
      # MDL < PQL 
      # [ ] find how to incorporate RL
      # NOTE keep those reported as below MDL because their estimate is still likely better
      MDL = mean(MDL, na.rm =T), 
      # PQL = mean(PQL, na.rm =T),  dropped PQL might need to add back imputeMethodn
      RL = mean(RL, na.rm =T), 
      RESULT = dplyr::case_when(
        !is.na(RESULT) ~ RESULT,
        # NOTE that taking the min of results would also include estimated values where they exist
        # Follow up- this means that values aren't consistently imputed when comparing those that are 
        # reported as estimates and what this program imputes
        grepl("N", Unified_Flag) & (!is.na(MDL) | sum(!is.na(RESULT)) > 1) ~ imputeFunction(min(MDL, RL, RESULT, na.rm=T)),
        .default = RESULT
      ),
      # make sure to manage different mdls over studies, times, and analytes
      .by = c(Study, Year, CodeName)
    ) %>%
    dplyr::mutate(
      # same thing but sometimes RL isn't availble for a given year so share across the years
      MDL = mean(MDL, na.rm =T), 
      # PQL = mean(PQL, na.rm =T),  dropped PQL might need to add back imputeMethodn
      RL = mean(RL, na.rm =T), 
      RESULT = dplyr::case_when(
        !is.na(RESULT) ~ RESULT,
        # NOTE that taking the min of results would also include estimated values where they exist
        # Follow up- this means that values aren't consistently imputed when comparing those that are 
        # reported as estimates and what this program imputes
        grepl("N", Unified_Flag) & (!is.na(MDL) | sum(!is.na(RESULT)) > 1) ~ imputeFunction(min(MDL, RL, RESULT, na.rm=T)),
        .default = RESULT
      ),
      # make sure to manage different mdls over studies, times, and analytes
      .by = c(Study, CodeName)
    )

  return(df)
}

df <- .dlImputation(df, imputeMethod = "uniform") %>%
  mutate(
    RESULT = ifelse(is.na(RESULT) & !is.na(RL), RL/2, RESULT)
  ) %>%
  filter(
    # remove 2 secchi samples that have an unrelated flag
    !(is.na(RESULT) & Unified_Flag == "T")
  )

# [ ] count imputed mdl vs rl
# [ ] Metals are missing mdls and rls
  # GLENDA has something going on leading to too many NAs
  # [ ]  figure out what it is 
expectedRows <- df %>%
  unite(un, sampleDateTime, Latitude, Longitude, sampleDepth) %>%
  mutate(wide_rows = length(unique(un))) %>%
  distinct(wide_rows) %>%
  pull(wide_rows)

df %>% 
  reframe(
    measureCount = sum(!is.na(RESULT)),
    missingCount = sum(is.na(RESULT)),
    #expectedMissing = expectedRows - measureCount,
    .by = CodeName
  ) %>%
  arrange(desc(missingCount), desc(measureCount)) %>%
  gt::gt() %>%
  gt::tab_header("Post DL imputation")
```


# Pivoting from wide to long

## Exact match pivoting
```{r}
df_wide <- df %>%
  # excludes units, flags, etc. to keep neat and since its in documentation
  # [ ] append units to codename
  pivot_wider(
    id_cols = c(Latitude, Longitude, sampleDateTime, sampleDepth, stationDepth, SITE_ID),
    names_from = c(CodeName, TargetUnits),
    values_from = RESULT,
    values_fn = function(x) mean(x,na.rm = T)
    ) %>%
  tidyr::drop_na(Latitude, sampleDateTime, sampleDepth)

df_wide %>%
  reframe(across(everything(), function(x) sum(is.na(x)))) %>%
  gt::gt() %>%
  gt::tab_header("Column Missingness")
```

## Closeness of nonmissing
```{r}
dayThres <- 3 
latlonThres <- 0.01

# [ ] keep site from chla observations
# [ ] don't just match to missing dataframe, rather the full dataframe minus the observation under question
chlamissing <- df_wide %>% filter(is.na(Chla))
chlanonmissing <- df_wide %>% drop_na(Chla) 

missingobs <- chlamissing %>% filter(sampleDateTime > ymd("2010-07-01")) %>% slice(1)

# different threshold depending on depth
depthThres <- ifelse(missingobs$sampleDepth > 20, 30, 5)
test <- chlanonmissing %>%
  # only look within some box
  dplyr::filter(
    dplyr::between(sampleDateTime, 
      missingobs$sampleDateTime - days(dayThres),
      missingobs$sampleDateTime + days(dayThres)
      ),
    dplyr::between(Latitude, 
      missingobs$Latitude - latlonThres,
      missingobs$Latitude + latlonThres
      ),
    dplyr::between(sampleDepth, 
      missingobs$sampleDepth - depthThres,
      missingobs$sampleDepth + depthThres
      ),
  # calculate multivariate distance
  ) %>%
  dplyr::mutate(
    dlat = missingobs$Latitude - Latitude,
    dlng = missingobs$Longitude - Longitude,
    d = sqrt(dlat **2 + dlng ** 2),
    # DISTANCE FOR DEPTH
    # # mean is shape * scale
    # # variance is shape * scale ** 2
    # expand.grid(q = 1:60, shape = 10, scale = 1.5) %>%
    #   mutate(
    #     cu = paste(shape, scale),
    #     p = pgamma(q, shape, scale = scale)
    #     ) %>%
    #   ggplot(aes(y = q, x = p, col = cu)) +
    #   geom_line()  +
    #   scale_y_reverse() +
    #   scale_x_reverse()
    #######################
    ddepth = abs(
      pgamma(sampleDepth, 10, scale = 1.5) - pgamma(missingobs$sampleDepth, shape = 10, scale = 1.5 )),
    ddays = sampleDateTime - missingobs$sampleDateTime,
    # compute a scalar distance (formula up for debate)
    # standardizing by 3 day, 20km cutoff
    D = (1/latlonThres * d) + (1 / depthThres) * ddepth + (1/dayThres) * abs(ddays)
  ) %>%
  # select minimum on this distance metric
  dplyr::filter(D == min(D, na.rm = T))
```

```{r}
dayThres <- 3 
latlonThres <- 0.01

# [ ] keep site from chla observations
# [ ] don't just match to missing dataframe, rather the full dataframe minus the observation under question
chlamissing <- df_wide %>% filter(is.na(Chla))
chlanonmissing <- df_wide %>% drop_na(Chla) 

missingobs <- chlamissing %>% filter(sampleDateTime > ymd("2010-07-01")) %>% slice(1)


.imputeNearestMatch <- function(..., matchingSet = NULL, dayThresh = 3, latlonThres = 0.01, CodeName = "Chla"){
  observation <- tibble::tibble(...)
  # different threshold depending on depth
  # depth Threshold could defintiely be done better using asymetric bound
  depthThres <- ifelse(observation$sampleDepth > 30, 15, 5)
  nearestMatch <- matchingSet %>%
    # only look within some box
    dplyr::filter(
      dplyr::between(sampleDateTime, 
        observation$sampleDateTime - days(dayThres),
        observation$sampleDateTime + days(dayThres)
        ),
      dplyr::between(Latitude, 
        observation$Latitude - latlonThres,
        observation$Latitude + latlonThres
        ),
      dplyr::between(sampleDepth, 
        observation$sampleDepth - depthThres,
        observation$sampleDepth + depthThres
        ),
    # calculate multivariate distance
    )
    if (nrow(nearestMatch) != 0) {
      nearestMatch <- nearestMatch %>%
      dplyr::mutate(
        dlat = observation$Latitude - Latitude,
        dlng = observation$Longitude - Longitude,
        d = sqrt(dlat **2 + dlng ** 2),
        # DISTANCE FOR DEPTH
        # # mean is shape * scale
        # # variance is shape * scale ** 2
        # expand.grid(q = 1:60, shape = 10, scale = 1.5) %>%
        #   mutate(
        #     cu = paste(shape, scale),
        #     p = pgamma(q, shape, scale = scale)
        #     ) %>%
        #   ggplot(aes(y = q, x = p, col = cu)) +
        #   geom_line()  +
        #   scale_y_reverse() +
        #   scale_x_reverse()
        #######################
        ddepth = abs(
          pgamma(sampleDepth, 10, scale = 1.5) - pgamma(observation$sampleDepth, shape = 10, scale = 1.5 )),
        ddays = sampleDateTime - observation$sampleDateTime,
        # compute a scalar distance (formula up for debate)
        # standardizing by 3 day, 20km cutoff
        D = ((1/latlonThres * d) + (1 / depthThres) * ddepth + (1/dayThres) * abs(ddays))
      ) %>%
      # select minimum on this distance metric
      dplyr::filter(D == min(D, na.rm = T)) %>%
      dplyr::slice(1) %>%
      dplyr::pull(CodeName)
      # if nothing within tolerance return NA
    } else {
      nearestMatch <- NA
    }
    return(nearestMatch)
}

# Time difference of 3.323348 mins
start.time <- Sys.time()
chlaMatched <- chlamissing %>%
  dplyr::rowwise() %>%
  dplyr::mutate(Chla = .imputeNearestMatch(Latitude, Longitude, sampleDepth, sampleDateTime,
   matchingSet = matchingSet, dayThresh = 3, latlonThres = 0.01, CodeName = "Chla")) %>%
   drop_na(Chla)
tempmissing <- df_wide %>% filter(is.na(Temp))
tempnonmissing <- df_wide %>% drop_na(Temp) 
tempMatched <- tempmissing %>%
  dplyr::rowwise() %>%
  dplyr::mutate(
    Temp1 = .imputeNearestMatch(Latitude, Longitude, sampleDepth, sampleDateTime,
   matchingSet = tempnonmissing, dayThresh = 3, latlonThres = 0.01, CodeName = "Temp"),
    Temp2 = .imputeNearestMatch(Latitude, Longitude, sampleDepth, sampleDateTime,
   matchingSet = tempnonmissing, dayThresh = 4, latlonThres = 0.01, CodeName = "Temp"),
    Temp3 = .imputeNearestMatch(Latitude, Longitude, sampleDepth, sampleDateTime,
   matchingSet = tempnonmissing, dayThresh = 4, latlonThres = 0.02, CodeName = "Temp"),
   )


end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

tempMatched %>%
  ungroup() %>%
  reframe(
    across(c(Temp1, Temp2, Temp3),
    .fns = list(added = function(x) sum(is.na(x)), prop_added = function(x) mean(is.na(x)))
  ))

```
- Takes 3.3 minutes for Chla column (25secs for Temp)
  - will potentially be longer for columns with greater missingness 
    - more matches to make
    - fewer boxes with an observation within
- 49 variables
  - 49 x 3.3  = 161 mins

Bring matched data back in
```{r}


```



```{r}

.dlImputation <- function(df, latlonDigits = 4, imputeMethod = NULL, timeBinning = "hours", depthCuts = NULL, 
  startDate = "2000-01-01", endDate = "2050-01-01"){
  removedDup <- df %>%  # removes 10% 
    dplyr::mutate(
      Lat = round(Latitude, digits = latlonDigits),
      Lng = round(Longitude, digits = latlonDigits),
      sampleDateTime = lubridate::round_date(sampleDateTime, unit = timeBinning),
    ) %>%
    dplyr::filter(
      dplyr::between(sampleDateTime, 
      lubridate::ymd(startDate),
      lubridate::ymd(endDate)
    )) %>%
    {if (!is.null(depthCuts)){
      dplyr::mutate(.,
        sampleDepth = cut(sampleDepth, breaks= depthCuts)
      )
    } else .}  %>%

  # pivot into wide format
  tabledData <- removedDup %>%
    pivot_wider(id_cols = c(Lat, Lng, sampleDateTime, sampleDepth), names_from = CodeName, values_from = Result)
  return(tabledData)
}


```



# Naive imputation
- roughly 2 minutes
```{r}
# Count of missing values per row
hist(rowSums(is.na(df_wide)))
hist(colSums(is.na(df_wide)))

# Subset rows with 2+ missing values 
test <- df_wide[, colMeans(is.na(df_wide)) < mean(is.na(df_wide$Chla)) * 1.25]

# missForest doesn't like tibbles so convert to deprecated df
test <- test %>% 
  select(CaCO3:CPAR) %>% 
  drop_na(Chla) %>% 
  data.frame()

# [ ] could stratify by near / offshore or seasonality
start.time <- Sys.time()
test2 <- missForest(test, variablewise = T)
colMeans(is.na(test2[[2]]))
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

length(sqrt(test2$OOBerror)) /
test %>% 
  reframe(across(everything(), function(x) sd(x, na.rm = T)))
```
- 6 minutes

```{r}
lm(Chla ~ ., data = test )
```

---
title: "Data Checks"
date: today
---

```{r setup}
library(tidyverse)
devtools::load_all()
df <- assembleData(.test = F, out = "fullData", binaryOut = TRUE)
#df <- readRDS("fullData.Rds")
```

# Missingness
```{r}
df %>%
  select(1:9, 16:21, 23:24,27) %>%
  reframe(across(everything(), function(x) mean(is.na(x))), .by = Study) %>%
  kable(digits=2) %>%
  kableExtra::kable_paper()
```

Notes on missingness:
- GLENDA
  - Has ~50% missingness
- SeaBird
  - Can't locate station depth in their files
- NCCA
  - Secchi missing sample depth because it is an integrated sample (over water column)

- CSMI
  - 2010
    - Lat/Lon reported only to nearest whole number 
      - there are real Lat/ real Lon in smpstts10.xls, but not sure what they mean
    - Lat/Lon/depth/sampleDate partially missing because odd reporting in file
      - Can we use forward filling to fill in the missing values?
    - Can't find sampleDepth
      - Just report the water layer

  - 2015 
    - It actually just has a lot of missing WQ 
      - I'm guessing this is because if CTD was measured and WQ wasn't they still record it as a NA
  - 2021
    - actually has ~50% missingness


```{r}
GLENDA %>%
  filter(Study == "GLENDA", is.na(RESULT)) %>%
  count(CodeName, ConversionFactor, is.na(as.numeric(RESULTstart))) %>%
  print(n = 64)
```

# Check CodeName Missingness

# Check CodeName / Analyte combos


# Check Units missingness


# Check target/rpoerted units and conversion factor


# Position missingness


# Full table of missingness

# Missingness by data source

# Flags
```{r flags}
flags <- df %>% 
  distinct(Study, QAcode, QAcomment) %>%
  separate_longer_delim(QAcode, delim = ";") %>%
  separate_longer_delim(QAcode, delim = ",") %>%
  separate_longer_delim(QAcomment, delim = ";") %>%
  distinct() %>%
  mutate(
    QAcode = str_remove_all(QAcode, "NA,"),
    QAcode = str_remove_all(QAcode, " NA,"),
    QAcomment = str_remove_all(QAcomment, "NA,"),
    QAcomment = str_remove_all(QAcomment, ", NA"),
    QAcomment = str_remove_all(QAcomment, " NA"),
    QAcode = str_trim(QAcode),
    QAcomment = str_trim(QAcomment),
    QAcode = ifelse(grepl("^NA", QAcode, ignore.case =T), NA, QAcode),
    QAcomment = ifelse(grepl("^NA", QAcomment, ignore.case =T), NA, QAcomment),
    QAcode = str_trim(QAcode),
    QAcomment = str_trim(QAcomment),
    ) %>%
  filter((!is.na(QAcode))  | (!is.na(QAcomment))) %>%
    dplyr::mutate(
      QAcode = ifelse((is.na(QAcode)) & (grepl("no reported units", QAcomment, ignore.case = T)),
      "U", QAcode),
      QAcomment = ifelse((QAcode == "U") & is.na(QAcomment),
      "No reported units, so assumed most common units for this given analyte-year", QAcomment)
    ) %>%
    distinct()
write_csv(flags, "tempFlags.csv")

```

### rounding lat lon
```{r}
rounded <- df %>%
  distinct(Study, SITE_ID, Latitude, Longitude) %>%
  reframe(studies = toString(unique(Study)), .by = c(SITE_ID, Latitude, Longitude)) %>%
  mutate(
    Lat2 = round(Latitude, digits=2),
    Lat3 = round(Latitude, digits=3),
    Lat4 = round(Latitude, digits=4),
    Lat5 = round(Latitude, digits=5),
    Long2 = round(Longitude, digits=2),
    Long3 = round(Longitude, digits=3),
    Long4 = round(Longitude, digits=4),
    Long5 = round(Longitude, digits=5),
    )

rounded2 <- rounded %>% 
  filter(n() > 1, .by =c(Lat2,Long2)) %>%
  reframe(
    sites = toString(unique(SITE_ID)),
    studies = toString(unique(studies)),
    .by = c(Lat2, Long2)
  )
rounded3 <- rounded %>% 
  filter(n() > 1, .by =c(Lat3,Long3)) %>%
  reframe(
    sites = toString(unique(SITE_ID)),
    studies = toString(unique(studies)),
    .by = c(Lat3, Long3)
  )
rounded4 <- rounded %>% 
  filter(n() > 1, .by =c(Lat4,Long4)) %>%
  reframe(
    sites = toString(unique(SITE_ID)),
    studies = toString(unique(studies)),
    .by = c(Lat4, Long4)
  )
rounded5 <- rounded %>% 
  filter(n() > 1, .by =c(Lat5,Long5)) %>%
  reframe(
    sites = toString(unique(SITE_ID)),
    studies = toString(unique(studies)),
    .by = c(Lat4, Long4)
  )

write_csv(rounded2, "2digitStationOverlap.csv")
write_csv(rounded3, "3digitStationOverlap.csv")
write_csv(rounded4, "4digitStationOverlap.csv")
write_csv(rounded5, "5digitStationOverlap.csv")
```

```{r}
unzip("ne_50m_lakes.zip")
theme_set(theme_bw())
lakes <- sf::read_sf("ne_50m_lakes.shp")

windows();ggplot(lakes) +
  geom_sf(fill = "lightblue") +
  coord_sf(xlim = c(-88, -85), ylim = c(41.5, 46.2)) +
  theme(panel.background = element_rect(fill = '#d0d890'),
        panel.grid = element_line(color = '#00000010')) +
  geom_point(data= rounded, aes(x = Long2, y = Lat2, size = 1e-2), col = "red",  alpha = 0.2)+
  geom_point(data= rounded, aes(x = Long3, y = Lat3, size = 1e-3), col = "yellow",  alpha = 0.2)+
  geom_point(data= rounded, aes(x = Long4, y = Lat4, size = 1e-4), col = "black",  alpha = 0.2) 
  scale_radius(range = range(c(1e-2, 1e-4)/2))
ggsave("differencePrecisionsLM.png")

windows();ggplot(lakes) +
  geom_sf(fill = "lightblue") +
  coord_sf(xlim = c(-87, -86), ylim = c(44, 45)) +
  theme(panel.background = element_rect(fill = '#d0d890'),
        panel.grid = element_line(color = '#00000010')) +
  geom_point(data= rounded, aes(x = Long2, y = Lat2, size = 1e-2), col = "red",  alpha = 0.2)+
  geom_point(data= rounded, aes(x = Long3, y = Lat3, size = 1e-3), col = "yellow",  alpha = 0.2)+
  geom_point(data= rounded, aes(x = Long4, y = Lat4, size = 1e-4), col = "black", alpha = 0.2) 
  scale_radius(range = range(c(1e-2, 1e-4)))
ggsave("differencePrecisionsLMzoom.png")

windows();ggplot(lakes) +
  geom_sf(fill = "lightblue") +
  coord_sf(xlim = c(-85.9, -85.3), ylim = c(44.7, 45.4)) +
  theme(panel.background = element_rect(fill = '#d0d890'),
        panel.grid = element_line(color = '#00000010')) +
  geom_point(data= rounded, aes(x = Long2, y = Lat2, size = 1e-2), col = "red",  alpha = 0.2)+
  geom_point(data= rounded, aes(x = Long3, y = Lat3, size = 1e-3), col = "yellow",  alpha = 0.2)+
  geom_point(data= rounded, aes(x = Long4, y = Lat4, size = 1e-4), col = "black", alpha = 0.2) 
  scale_radius(range = range(c(1e-2, 1e-4)))
ggsave("differencePrecisionsLMNOAA.png")


stations %>%
  mutate(nch = nchar(as.character(Latitude))) %>%
  count(nch)

```

# Duplicates
```{r duplicates}

removedDup <- df %>%  # 11877651 partial observations
  dplyr::distinct() %>% # 11853750 
  # [ ] Round to that decimal place and do exact match
  # [ ] Include study as another column
  # [ ] For each lat/lon write list of stations
    # [ ] Then do an exact match


suspectedDup %>%
  select(Study, Latitude, Longitude, sampleDepth, CodeName, sampleDateTime, RESULT)


```

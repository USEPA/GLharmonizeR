---
title: "GLENDA data cleaning description"
metadata-files:
  - "../../_quarto.yml"
toc: true
---

```{css scrollableTables, echo = FALSE}
.table {
max-height: 500px;
overflow: auto;
}
```


# Load and setup

* Data sources
  * [GLENDA](https://cdxapps.epa.gov/cdx-glenda/action/querytool/querySystem) 
  * [QA codes](https://cdxapps.epa.gov/cdx-glenda/action/linkdownloads/ExcelDownloads?downloadFile=&uploadFileId=964) taken from 2019 prepackaged data, from the "Qualifiers" sheet. Note: Didn't use "QC codes" sheet because couldn't find where this would map to the dataset 
  * Didn't utilize Depth codes from prepackaged workbook either  

There were a couple initial filter choices when we initially loaded the data for exploration. 

* Pivot the data into a long format
* Enforce stict data types for each column
* SAMPLE TYPES were either "Individual" or "INSITU_MEAS"
* QC TYPEs were "routine field sample"
* Dropped observations where both VALUE and RESULT_REMARK are missing
  * This is before enforcing VALUE is a number, so we wouldn't induce a missing value
  * We will deal with the induced NA's on the basis of the RESULT_REMARK
* Inferred inclusion risks for each RESULT_REMARK (see later in report)
* force Values to be numeric. Note: Some of them had words in them if they were questionable



```{r setupNload}
#| include: false
source("Code/dataCleaning/readCleanGLENDA.R")
library(GGally)
library(cluster)
library(ggfortify)
library(kableExtra)
library(ggbeeswarm)
library(gt)

df <- readCleanGLENDA(filepath = "Data/Raw/GLENDA/GLENDA.csv", flagsPath = NULL, siteCoords = NULL, imputeCoordinates= FALSE, nameMap = NULL)
#remarkCodes <- read_csv("Data/Raw/GLENDA/remarkCodes.csv")
# QCCodes <- read_csv("Data/Raw/GLENDA/QCcodes.csv")
#depthCodes <- read_csv("Data/Raw/GLENDA/DepthCodes.csv")
```

::: {.table}
```{r}
#| label: tbl-fullData
#| tbl-cap: "First 10 rows of the full dataset after pivoting and performing selections outline above"

head(df) %>%
  gt() 
```
:::

# QC investigations
## Fractions
- Some fractions are reported missing
- Here are the suggested fractions, some are still unknown (Note: Adding unique Method didn't yield new matches, that weren't themselves missing)
- So far, this was purely based about solubility of the analytes

```{r fractions}
#| label: tbl-fractionLabels
#| tbl-cap: "Giving unreported fractions fraction labels"


df %>%
  distinct(ANALYTE, FRACTION)  %>%
  group_by(ANALYTE) %>%
  filter(any(is.na(FRACTION)) | any(FRACTION == "Not applicable"))  %>% 
  mutate(suggestedFraction = case_when(
    grepl("CaCO3", ANALYTE) ~ "Total/Bulk",
    (grepl("Chloro", ANALYTE)) & (is.na(FRACTION)) ~ "",
    grepl("Conductivity", ANALYTE) ~ "Total/Bulk",
    grepl("Secchi", ANALYTE) ~ "Total/Bulk",
    ANALYTE == "Temperature" ~ "Total/Bulk",
    ANALYTE == "Turbidity" ~ "Total/Bulk",
    ANALYTE == "pH" ~ "Total/Bulk"
  )) %>%
  arrange(ANALYTE, FRACTION) %>%
  ungroup() %>%
  gt() %>%
  tab_header(
    title = md("Unknown fractions and their suggested labels"),
    subtitle = md("Tablation of unique NA or Not applicable fractions and their suggested labels")
  )
```
:::

## NAs, Remarks, and problematic values

::: {.table}
```{r remarkDecision}
#| label: tbl-remarks
#| tbl-cap: "Listing all of the remarks and whether the values were reported or just NA"

naTreatment <- readPivotGLENDA("Data/Raw/GLENDA/GLENDA.csv") %>%
  separate_longer_delim(RESULT_REMARK, delim = ";") %>%
  drop_na(RESULT_REMARK)  %>%
  mutate(VALUE = case_when(as.numeric(VALUE) == 0 ~ as.character(0),
                           as.numeric(VALUE) < 0 ~ "Negative",
                           as.numeric(VALUE) > 0  ~ "Reported",
                           grepl("<", VALUE) ~ "<",
                           is.na(as.numeric(VALUE)) ~ "Not Reported"
                           )) %>%
  distinct(VALUE, RESULT_REMARK) %>%
  mutate(treatment = case_when(
    # Most extreme up top to be more conserivative 
    grepl("bias", RESULT_REMARK, ignore.case = T) ~ "Remove",
    grepl("Inconsistent", RESULT_REMARK, ignore.case = T) ~ "Remove", 
    is.na(RESULT_REMARK) ~ "Remove",
    grepl("Invalid", VALUE, ignore.case=T) ~ "Remove",
    grepl("Invalid", RESULT_REMARK, ignore.case=T) ~ "Remove",
    grepl("no result", RESULT_REMARK, ignore.case = T) ~ "Remove",
    grepl("fail", RESULT_REMARK, ignore.case= T) ~ "Remove",
    VALUE == "NRR" ~ "Remove",
    VALUE %in% c("T", "W") ~ "Remove",
    grepl("anomaly", RESULT_REMARK, ignore.case= T) ~ "Remove",
    grepl("contaminat", RESULT_REMARK, ignore.case= T) ~ "Remove",
    grepl("incomplete", RESULT_REMARK, ignore.case= T) ~ "Remove",
    grepl("Holding time", RESULT_REMARK, ignore.case= T) ~ "Remove",
    grepl("null", RESULT_REMARK, ignore.case= T) ~ "Remove",
    grepl("Outlier", RESULT_REMARK, ignore.case= T) ~ "Remove",
    grepl("fail", RESULT_REMARK, ignore.case = T) ~ "Remove",
    VALUE == "INV" ~ "Remove",
    (VALUE == "Reported") & grepl("limit", RESULT_REMARK, ignore.case = T) ~ "Impute",
    grepl("limit", RESULT_REMARK, ignore.case = T) ~ "Impute",
    grepl("Outli", RESULT_REMARK, ignore.case = T) ~ "Impute",
    grepl("<", VALUE) ~ "Impute", # Maybe just remove the less than?
    grepl("less", RESULT_REMARK, ignore.case = T) ~ "Keep",
    grepl("estimate", RESULT_REMARK, ignore.case= T) ~ "Keep",
    grepl("correction", RESULT_REMARK, ignore.case= T) ~ "Keep",
    (VALUE == "Reported")  & grepl("limit", RESULT_REMARK, ignore.case = T) ~ "Keep",
    VALUE == "Not Reported" ~ "Remove",
  )) %>%
  select(VALUE, treatment, RESULT_REMARK) %>%
  arrange(VALUE, treatment) 

naTreatment %>%
  gt() %>%
  tab_header(
    title = "Missing/estimated/Measured values",
    subtitle = "Types of reported values and their treatment"
  )
```

:::

## Units
- For mismatched units:
  - For each Unique analyte, Fraction, and whether below 20 or not
- Saw if distributions differ
- Assume internally consistent
- Test log of distance between all pairwise combinations
- Identified mismatched units by testing if their means are significantly different
- labeled in the UNIT_DETERMINATION column

```{r unitDistributions}
df %>%
  mutate(Above = SAMPLE_DEPTH_M <20) %>%
  drop_na(Above) %>%
  ggplot(aes(y = VALUE, fill = UNITS, group = UNITS)) +
    geom_histogram(alpha = 0.3, position = "identity") +
    ggh4x::facet_grid2(rows = vars(ANALYTE), cols = vars(Above), scales="free", independent = "all", remove_labels = "all", render_empty = F,
    ) +
    scale_x_log10() +  
    theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=  element_blank(),
        axis.ticks.y=element_blank() ) + 
  ggtitle("Distributions of Analytes above (Right) and below (Left) 20 meters")
```

```{r identifyUnits}
unitMatchingness <- df %>%
  mutate(Above = SAMPLE_DEPTH_M <20) %>%
  group_by(ANALYTE, FRACTION, UNITS, Above) %>%
  reframe(med = median(VALUE, na.rm =T),
          METHOD = toString(unique(METHOD))) %>% 
  group_by(ANALYTE, Above) %>%
  reframe(
    d = mean(abs(diff(log10(med))), na.rm= T),
    Units = toString(unique(UNITS)
    )) %>%
  mutate(UNIT_DETERMINATION = case_when(
    d<0.33 ~ "Same",
    between(d, 0.33, 2.5) ~ "Need to Check",
    d > 2.5 ~ "Different"
  )) 
unitMatchingness %>%
  ggplot(aes(x = d, fill = UNIT_DETERMINATION)) +
  geom_histogram()
```


## LAT/LON precision
- Lat/Lon have varying degrees of precision. Is this from rounding?
- Missing from pre 1994
- How far is tolerable for distance from Lat/Lon center? Some are as big as 1000 meters
```{r checkLatLon}
df %>% 
  mutate(
    LATITUDE = as.character(LATITUDE),
    lat_dec = nchar(str_split_i(LATITUDE, "\\.", i =2)),
    LONGITUDE = as.character(LONGITUDE),
    lon_dec = nchar(str_split_i(LONGITUDE, "\\.", i = 2))) %>%
  select(lat_dec, lon_dec) %>%
  pivot_longer(cols = c(lat_dec, lon_dec), values_to = "decimals", names_to = "coord") %>%
  ggplot(aes(x = decimals, fill = coord)) +
  geom_bar(position = "dodge") + 
  ggtitle("Decimal places for coordinates")
```

```{r checkLatLon2}
df %>% 
  mutate(
    LATITUDE = as.character(LATITUDE),
    lat_dec = nchar(str_split_i(LATITUDE, "\\.", i =2)),
    LONGITUDE = as.character(LONGITUDE),
    lon_dec = nchar(str_split_i(LONGITUDE, "\\.", i = 2))) %>%
  select(YEAR, lat_dec, lon_dec) %>%
  pivot_longer(cols = c(lat_dec, lon_dec), values_to = "decimals", names_to = "coord") %>%
  ggplot(aes(x = YEAR, y = decimals, col = coord)) +
  geom_jitter() +
  geom_smooth(method = "lm", se = T) + 
  ggtitle("Decimal places for coordinates")
```



```{r latLongPerYear}

df %>%
  group_by(YEAR) %>%
  reframe(p_missing_lon = mean(is.na(LONGITUDE))) %>%
  ggplot(aes(x = YEAR, y = p_missing_lon)) +
  geom_point(size = 5) +
  ggtitle("Coordinates missing % per year")


df %>% 
  select(STATION_ID, LATITUDE, LONGITUDE) %>%
  group_by(STATION_ID) %>%
  mutate(meanLat = mean(LATITUDE, na.rm = T), meanLon = mean(LONGITUDE, na.rm = T),
         meters = sqrt((LATITUDE - meanLat)^2 + (LONGITUDE - meanLon)^2),
         meters = meters * 111320) %>%
  ggplot(aes(x = meters)) +
  geom_histogram() + 
  scale_x_log10() +
  ggtitle("Sample distance from Station Site mean")
```

## Date and time distributions
```{r times}
df %>%
  distinct(sampleDate, SAMPLE_ID) %>%
  ggplot(aes(x = sampleDate)) +
  geom_histogram()

# Time of day vs year
df %>%
  distinct(sampleDate, SAMPLE_ID) %>%
  mutate(year= year(sampleDate), tod = hour(sampleDate)) %>%
  ggplot(aes(x = year, y = tod)) +
  geom_jitter() +
  geom_smooth()

# Date vs year
df %>%
  distinct(sampleDate, SAMPLE_ID) %>%
  mutate(year= year(sampleDate), day = yday(sampleDate)) %>%
  ggplot(aes(x = year, y = day)) +
  geom_jitter() +
  geom_smooth()
```


## Analyte names
- Named by hand after looking at the following table 

::: {.table}
```{r namingTable}
#| label: namingTable
#| tbl-cap: "A list of all unique Medium, Analyte, Fraction combinations to aid in determining a unified naming system. Additionally, we've included columns for different attempts at creating a naming system"


# Read existing names, so we don't lose that work
existingNames <- readxl::read_xlsx("Analytes2.xlsx", sheet="Unifying Names", skip = 0)%>%
  select(MEDIUM, ANL_CODE, FRACTION, `Suggested name`, NAME_COMMENT) %>%
  mutate(FRACTION = ifelse(FRACTION == "Not applicable", "", FRACTION)) 

# Make a table for naming
namingTable <- df %>%
  group_by(MEDIUM, ANALYTE, ANL_CODE, FRACTION) %>%
  reframe(min = min(YEAR, na.rm=T), max = max(YEAR, na.rm = T),
          METHOD = toString(unique(METHOD)),
          UNITS = toString(unique(UNITS))) %>%
  unite(Years, min, max, sep = "-", remove = T) %>%
  left_join(existingNames, by = c("MEDIUM", "ANL_CODE", "FRACTION")) 

namingTable %>%
  gt()
```
:::

### Compiling tables
```{r writingXcel}
# filterDecisions <- readxl::read_xlsx("Analytes2.xlsx", sheet =1)

# writexl::write_xlsx(list("Filter Decisions" = filterDecisions, "Complete Data" = df, "Unifying Names" = namingTable, "Unifying Units" = unitMatchingness, "NA Treatment" = naTreatment), path = "Analytes2.xlsx")
```

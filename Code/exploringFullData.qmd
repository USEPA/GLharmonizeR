---
title: "Exploring Full WQD"
format: 
    html: 
      html-math-method: katex
      embed-resources: true
execute:
    message: false
    warning: false
    error: false
    echo: false
editor: visual
---

I split the NA types into three categories (for sure NA's, Unsure NA or not, Definitely not NA's but I'll address later). You can see with the line break in na argument. I determined missing values in each "VALUE" column with the commented out code below Dropped ANAL_CODE\_ columns because they are redundant with the "Analyte" columns

```{r setupNload}
#| include: false
library(tidyverse)
library(GGally)
library(naniar)
library(caret)
library(randomForest)
library(janitor)
#library(ggh4x) # rescale facet plots
library(kableExtra)
library(sf) # 1.0-12
library(sp) # 1.4-5
library(rnaturalearth)
library(rnaturalearthdata)
library(ggbeeswarm)
library(visdat)
library(cluster)
library(ggfortify)

df <- read_csv("C:/Users/ccoffman/OneDrive - Environmental Protection Agency (EPA)/Profile/Documents/Projects/LM_Chla/Data/allWQ.csv", na = c("", "NA", "no result reported", "No result reported.", "No result recorded.", "No Result Reported", "No Result Reported.", "NO RESULT", "no result", "No result reported",
                                                                                                                             "T", "NRR",  "INV", "W", "INVALID 89", "INVALID 88", "INVALID 88.5", "LAC", "INVALID 115",  "INVALID 8.4", "INVALID 11.645993", "INVALID 680", "INVALID 17.179", "INVALID 26.8039", "INVALID 4.1", "INVALID 42.9", "INVALID 6.5",
                                                                                                                             "<0.12", "<0.1", "<500", "<2", "<1", "<0.05")) %>%
  # These columns are redundant with the "Analyte" columns
  select(-contains("ANL_CODE_"), -Row) %>%
  # Drop columns with 100% missingness
  #select_if(~mean(is.na(.)) < 0.9) %>%
  pivot_longer(cols = -c(1:18),
               names_to = c(".value", "Number"),
               names_pattern = "(.*)_(\\d*)$") %>%
  filter(!is.na(ANALYTE), 
         QC_TYPE == "routine field sample",
         SAMPLE_TYPE != "Composite") %>%
  mutate(SAMPLING_DATE = parse_datetime(SAMPLING_DATE, "%Y/%m/%d %H:%M")) %>%
  filter(! ANALYTE %in% c("Secchi Disc Transparency", # similar to turbidity and high missingness
                          "Suspended solids, total",  # similar to turbidity and high missingness 
                          "Hardness, Total as CaCO3" # similar to alkalinity and high missingness
                          ))
# Other filter ideas
# FIX TIME ZONES
# Depth Code 'thermocline


# head(sort(table(df$VALUE_1), decreasing=T), 200)
# This is where I determined the NA values
#Clear_NAs <- df$VALUE_9[!is.na(df$VALUE_9)]
#unique(Clear_NAs[is.na(as.numeric(Clear_NAs))])
#df %>% 
#  select(contains("VALUE")) %>%
#  glimpse()
# df %>% 
#   select(YEAR, STN_DEPTH_M, LATITUDE, LONGITUDE, SAMPLE_DEPTH_M, contains("VALUE_")) %>%
#   glimpse()
```

# Spatial summary
```{r samplesSpace}
#| fig-cap: "Density of samples with respect to latitude and longitude"
#| cache: true

theme_set(theme_bw())
# url <- paste0("https://www.naturalearthdata.com/", 
#               "http//www.naturalearthdata.com/download/10m/physical/",
#               "ne_10m_lakes.zip")
# 
# path <- tempdir()
# download.file(url, paste0(path, "/lakes.zip"))
# unzip(paste0(path, "/lakes.zip"))
lakes <- read_sf("ne_10m_lakes.shp")

ggplot(lakes) +
  geom_sf(fill = "lightblue") +
  coord_sf(xlim = c(-88, -85), ylim = c(41.5, 46.2)) +
  theme(panel.background = element_rect(fill = '#d0d890'),
        panel.grid = element_line(color = '#00000010')) +
  geom_density_2d(data= df, aes(x = LONGITUDE, y = LATITUDE))

```



```{r depths}
tempDF <- data.frame("Depth" = c(10, 40),
                     "Text" = c("Suggestion = 10m",
                                "Deepest thermocline (summer) = 13m"),
                     "Height" = c(200, 400)) 

df %>%
  distinct(SAMPLE_ID, .keep_all=T) %>%
  ggplot(aes(x = SAMPLE_DEPTH_M)) +
  geom_histogram() + 
  geom_vline(aes(xintercept = Depth), data= tempDF, color = "red") +
  geom_text(aes(x = Depth, y = Height, label = Text), data= tempDF)

```

## Time summary 
```{r sampleYears}
#| tbl-cap: "Range of years of sampling for each analyte colored by the proportion of zeros that were observed"
df %>%
 # mutate(ANALYTE = reorder(ANALYTE, MIN)) %>%
  mutate(VALUE = VALUE ==0) %>%
  group_by(YEAR, ANALYTE) %>%
  summarize(`Proportion 0` = mean(VALUE, na.rm = T)) %>% 
  ggplot(aes(x = YEAR,
             y = ANALYTE,
             col = `Proportion 0`)) +
  #geom_dotplot()
  #geom_quasirandom() 
  # geom_violin(
  geom_point(alpha= 0.8)
  
```

## Season summary
```{r sampleSeason}
#| tbl-cap: "Range of  months of sampling for each analyte colored by the proportion of zeros that were observed"
df %>%
 # mutate(ANALYTE = reorder(ANALYTE, MIN)) %>%
  mutate(VALUE = VALUE ==0) %>%
  group_by(MONTH, ANALYTE) %>%
  summarize(`Proportion 0` = mean(VALUE, na.rm = T)) %>% 
  mutate(MONTH = factor(MONTH, levels = c("January", "February", "March", "April", "June", "July", "August", "September", "October", "November", "December"))) %>%
  ggplot(aes(x = MONTH,
             y = ANALYTE,
             col = `Proportion 0`)) +
  #geom_dotplot()
  #geom_quasirandom() 
  # geom_violin(
  geom_point(alpha= 0.8)+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))

```
## Time of Day
```{r sampleTime}
#| tbl-cap: "Range of  months of sampling for each analyte colored by the proportion of zeros that were observed"
df %>%
 # mutate(ANALYTE = reorder(ANALYTE, MIN)) %>%
  mutate(Hour = hour(SAMPLING_DATE)) %>%
  ggplot(aes(x = Hour)) +
  geom_histogram()

```

# Sampling Fraction 
```{r}
#| tbl-cap: "Number of observations from each type of sampling fraction"
fractions <- df %>%
  count(ANALYTE, FRACTION) %>%
  group_by(ANALYTE) %>%
  mutate(total = sum(n))
fractions %>%
  ggplot(aes(x = ANALYTE, y =n, fill = FRACTION)) +
  geom_bar(stat = "identity") +
  geom_text( aes(label = ANALYTE, y = total), data = fractions[fractions$total > 5000 ,], angle =45, size = 2) + 
  theme(axis.title.x=element_blank(),
     axis.text.x=element_blank(),
     axis.ticks.x=element_blank())

```

# Depth Codes 
```{r}
#| tbl-cap: "Number of observations from each  Depth (represented by color)"
fractions <- df %>%
  count(ANALYTE, DEPTH_CODE) %>%
  group_by(ANALYTE) %>%
  mutate(total = sum(n))
fractions %>%
  ggplot(aes(x = ANALYTE, y =n, fill = DEPTH_CODE)) +
  geom_bar(stat = "identity") +
  geom_text( aes(label = ANALYTE, y = total), data = fractions[fractions$total > 2500 ,], angle =45, size = 2) + 
  theme(axis.title.x=element_blank(),
     axis.text.x=element_blank(),
     axis.ticks.x=element_blank(),
     legend.position = "none")

```



# Remarks 
```{r}
#| tbl-cap: "Counts for each type of remarks"
remarkCodes <- read_csv("C:/Users/ccoffman/OneDrive - Environmental Protection Agency (EPA)/Profile/Documents/Projects/LM_Chla/Data/remarkCodes.csv")
QCCodes <- read_csv("C:/Users/ccoffman/OneDrive - Environmental Protection Agency (EPA)/Profile/Documents/Projects/LM_Chla/Data/QCcodes.csv")
depthCodes <- read_csv("C:/Users/ccoffman/OneDrive - Environmental Protection Agency (EPA)/Profile/Documents/Projects/LM_Chla/Data/DepthCodes.csv")


remarks <- df %>%
  filter(!is.na(RESULT_REMARK)) %>% 
  count(RESULT_REMARK)%>%
  arrange(desc(n)) %>%
  left_join(remarkCodes, by= c("RESULT_REMARK"= "NAME"))

remarks %>%
  group_by(CLASS) %>%
  summarize(n = sum(n)) %>%
  mutate(CLASS = reorder(CLASS, n)) %>%
  ggplot(aes(x = CLASS, y = n)) +
  geom_bar(stat= "identity") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
  ggtitle("Classes of Remarks")

remarks %>%
  filter(!is.na(CLASS)) %>%
  ggplot(aes(x = RESULT_REMARK, y = n)) +
  geom_bar(stat=  "identity") +
  facet_wrap(~CLASS, scales = "free") + 
  geom_text( aes(label = RESULT_REMARK, y = n), angle =45, size = 2) + 
  theme(axis.text.x=element_blank(),
     axis.ticks.x=element_blank()) +
  ggtitle("Subclasses of Remarks")
```

# Missingness 
## Analytes
```{r MissingnessChloro}
#| warning: false
dfWide <- df %>%
  select(SAMPLE_ID, YEAR, SEASON, ANALYTE, VALUE) %>%
  distinct(SAMPLE_ID, ANALYTE, .keep_all = T) %>% 
  pivot_wider(id_cols = c(SAMPLE_ID, YEAR, SEASON), names_from = ANALYTE, values_from = VALUE) %>%
  filter(!is.na(`Chlorophyll-a`)) %>%
  janitor::clean_names()

naniar::gg_miss_var(dfWide, show_pct = T)
dfWide %>%
  mutate(ninetySizePlus = year > 1996) %>%
  naniar::gg_miss_var(show_pct = T, facet = ninetySizePlus) + 
  ggtitle("Pre/post '96")


```



## Spatial information 
```{r}
dfWide <- dfWide %>%
  select_if(~mean(is.na(.)) < 0.2)
df %>% 
  select(ANALYTE, LATITUDE, LONGITUDE, STN_DEPTH_M) %>%
  naniar::gg_miss_var(show_pct = T, facet = ANALYTE) + 
  ggtitle("Overall Missingness")

df %>% 
  filter(YEAR < 1996) %>%
  group_by(ANALYTE) %>%
  dplyr::select(LATITUDE, LONGITUDE, STN_DEPTH_M, ANALYTE) %>%
  naniar::gg_miss_var(show_pct = T) + 
  ggtitle("pre-'96 missingness") +
  facet_wrap(~ANALYTE)

```


# Modeling information
## Marginal correlations of selected covariates
```{r ggpairs}
#| warning: false
#| fig-caption: "Pairwise correlation for selected variables"
#| cache: true

# lModel <- lm(formula = `Chlorophyll-a` ~ .- SAMPLE_ID, data= dfWide)

ggpairs(dfWide, columns = 2:10, progress = F)



```

## RF Model
```{r splittingData}
set.seed(222)
dfWide <- janitor::clean_names(dfWide)
ind <- sample(2, nrow(dfWide), replace = TRUE, prob = c(0.7, 0.3))
train <- dfWide[ind==1,]
test <- dfWide[ind==2,]
```

```{r randomForest}
#| cache: true
#| tbl-cap: "Prediction results on hold out data"
rf <- randomForest(chlorophyll_a ~. - sample_id, data=train, proximity=TRUE, na.action = na.omit) 
print(rf)
p <- predict(rf, newdata = test)
postResample(p, test$chlorophyll_a) %>%
  as.data.frame() %>%
  kable() %>%
  kable_paper()
```

## Distributions of selected covariates
```{r covarDists}
g <- dfWide %>%
  pivot_longer(-c(sample_id, year, season)) %>%
  ggplot(aes(x = value)) +
  geom_histogram() +
  facet_wrap(~name, scales = "free") + 
  ggtitle("linear scale")
g
g+ scale_x_log10() +ggtitle("log10 scale")
```

```{r spaghetti}
df %>%
  filter(ANALYTE == "Chlorophyll-a", DEPTH_CODE == "Surface") %>%
  unite("Position", LATITUDE, LONGITUDE) %>%
  mutate(SAMPLING_DATE = date(SAMPLING_DATE)) %>%
  group_by(SAMPLING_DATE, Position) %>%
  summarize(CHLA = mean(VALUE, rm.na=  T)) %>%
  ggplot(aes(x = SAMPLING_DATE, y = CHLA, col = factor(Position))) +
  geom_line() +
  theme(legend.position = "none") +
  ggtitle("Spaghetti plot of CHLA over time for each LAT/LONG, Surface samples")

```

# Dealing with similar/same analytes
| Similar Analytes | How to resovlve | Reason |
|:---------------|:----------------|:-------|
|Ammonia-Nitrogen / Ammonium-Nitrogen | | |
| Alkalinity / Hardness | Drop Hardness | Hardness has 95% missingness |
| Nitrogen particulate / Nitrogen sediment | | |
| Turbidity / Secchi disc / total suspended solids | Keep Turbidity | Others have >90% missingness |

| Phosphorus, Base Extractable as P" ? ???? 
- Phosphorus Total as P
- Phsophorus particulate/ sediment

```{r}
dfWide %>%
  select(contains("Hardness"), contains("Alkalinity")) %>%
  visdat::vis_miss()

dfWide %>%
  select(contains("Turbid"), contains("Secchi"), contains("suspended")) %>%
  visdat::vis_miss()


```

# PCA 
```{r}
dfWideNoMiss <- dfWide %>%
  janitor::clean_names() %>%
  mutate(lchla = log(chlorophyll_a + 0.01)) %>%
  drop_na()
PCA <- dfWideNoMiss %>% 
  select(-c(sample_id, season, year, chlorophyll_a, lchla))  %>%
  prcomp(center = T, scale=T)

plot(summary(PCA)$importance[2,], xlab= "Principal component", ylab = "% variance explained")

autoplot(PCA, data = dfWideNoMiss, colour = "lchla",
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)
autoplot(PCA, data = dfWideNoMiss, colour = "season",
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)
autoplot(clara(dfWideNoMiss[-c(2,10)], 5))
```

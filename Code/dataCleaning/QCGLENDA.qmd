---
title: "GLENDA data cleaning description"
metadata-files:
  - "../../_quarto.yml"
execute: 
  cache: true
toc: true
---

# Load and setup

* Data sources
  * [GLENDA](https://cdxapps.epa.gov/cdx-glenda/action/querytool/querySystem) 
  * [QA codes](https://cdxapps.epa.gov/cdx-glenda/action/linkdownloads/ExcelDownloads?downloadFile=&uploadFileId=964) taken from 2019 prepackaged data, from the "Qualifiers" sheet. Note: Didn't use "QC codes" sheet because couldn't find where this would map to the dataset 
  * Didn't utilize Depth codes from prepackaged workbook either  

There were a couple initial filter choices when we initially loaded the data for exploration. 

* Pivot the data into a long format
* Enforce stict data types for each column
* SAMPLE TYPES were either "Individual" or "INSITU_MEAS"
* QC TYPEs were "routine field sample"
* Dropped observations where both VALUE and RESULT_REMARK are missing
  * This is before enforcing VALUE is a number, so we wouldn't induce a missing value
  * We will deal with the induced NA's on the basis of the RESULT_REMARK
* Inferred inclusion risks for each RESULT_REMARK (see later in report)
* force Values to be numeric. Note: Some of them had words in them if they were questionable



```{r setupNload}
#| include: false
source("Code/dataCleaning/readCleanGLENDA.R")
library(GGally)
library(cluster)
library(ggfortify)
library(kableExtra)
library(ggbeeswarm)
library(gt)


df <- readCleanGLENDA("Data/Raw/GLENDA/GLENDA.csv")
remarkCodes <- read_csv("Data/Raw/GLENDA/remarkCodes.csv")
# QCCodes <- read_csv("Data/Raw/GLENDA/QCcodes.csv")
#depthCodes <- read_csv("Data/Raw/GLENDA/DepthCodes.csv")
```


# QC investigations
## Fractions
- Some fractions are reported missing
- Here are the suggested fractions, some are still unknown (Note: Adding unique Method didn't yield new matches, that weren't themselves missing)
- So far, this was purely based about solubility of the analytes

```{r fractions}
#| label: tbl-fractionLabels
#| tbl-cap: "Giving unreported fractions fraction labels"


df %>%
  distinct(ANALYTE, FRACTION)  %>%
  group_by(ANALYTE) %>%
  filter(any(is.na(FRACTION)) | any(FRACTION == "Not applicable"))  %>% 
  mutate(suggestedFraction = case_when(
    grepl("CaCO3", ANALYTE) ~ "Total/Bulk",
    (grepl("Chloro", ANALYTE)) & (is.na(FRACTION)) ~ "",
    grepl("Conductivity", ANALYTE) ~ "Total/Bulk",
    grepl("Secchi", ANALYTE) ~ "Total/Bulk",
    ANALYTE == "Temperature" ~ "Total/Bulk",
    ANALYTE == "Turbidity" ~ "Total/Bulk",
    ANALYTE == "pH" ~ "Total/Bulk"
  )) %>%
  arrange(ANALYTE, FRACTION) %>%
  ungroup() %>%
  gt() %>%
  tab_header(
    title = md("Unknown fractions and their suggested labels"),
    subtitle = md("Tablation of unique NA or Not applicable fractions and their suggested labels")
  )
```

## NAs, Remarks, and problematic values
Different types of Not reported values
```{r notReportedValues, max.height='100px'}
#| label: tbl-notReportedValues
#| tbl-cap: "A list of all of the not reported values and their associated remarks"

testdf <- readPivotGLENDA("Data/Raw/GLENDA/GLENDA.csv") 

testdf %>% 
  filter(is.na(as.numeric(VALUE))) %>%
  mutate(VALUE = tolower(VALUE),
        VALUE = ifelse(grepl("no result", VALUE, ignore.case = T), "nrr", VALUE)) %>%
  distinct(VALUE, RESULT_REMARK) %>%
  arrange(VALUE) %>%
  gt() %>%
  tab_header(
    title = "Unique non Numerics",
    subtitle = "All of the nonnumeric and their associated remarks"
  )
```

```{r remarkDecision}
#| label: tbl-remarks
#| tbl-cap: "Listing all of the remarks and whether the values were reported or just NA"

naTreatment <- testdf %>%
  separate_longer_delim(RESULT_REMARK, delim = ";") %>%
  drop_na(RESULT_REMARK)  %>%
  mutate(VALUE = case_when(as.numeric(VALUE) == 0 ~ as.character(0),
                           as.numeric(VALUE) < 0 ~ "Negative",
                           as.numeric(VALUE) > 0  ~ "Reported",
                           is.na(as.numeric(VALUE)) ~ "Not Reported",
                           grepl("<", VALUE) ~ "<")) %>%
  distinct(VALUE, RESULT_REMARK) %>%
  mutate(treatment = case_when(
    # Most extreme up top to be more conserivative 
    grepl("bias", RESULT_REMARK, ignore.case = T) ~ "Remove",
    grepl("Inconsistent", RESULT_REMARK, ignore.case = T) ~ "Remove", 
    is.na(RESULT_REMARK) ~ "Remove",
    grepl("Invalid", VALUE, ignore.case=T) ~ "Remove",
    grepl("Invalid", RESULT_REMARK, ignore.case=T) ~ "Remove",
    grepl("no result", RESULT_REMARK, ignore.case = T) ~ "Remove",
    grepl("fail", RESULT_REMARK, ignore.case= T) ~ "Remove",
    VALUE == "NRR" ~ "Remove",
    VALUE %in% c("T", "W") ~ "Remove",
    grepl("anomaly", RESULT_REMARK, ignore.case= T) ~ "Remove",
    grepl("contaminat", RESULT_REMARK, ignore.case= T) ~ "Remove",
    grepl("incomplete", RESULT_REMARK, ignore.case= T) ~ "Remove",
    grepl("Holding time", RESULT_REMARK, ignore.case= T) ~ "Remove",
    grepl("null", RESULT_REMARK, ignore.case= T) ~ "Remove",
    grepl("Outlier", RESULT_REMARK, ignore.case= T) ~ "Remove",
    grepl("fail", RESULT_REMARK, ignore.case = T) ~ "Remove",
    VALUE == "INV" ~ "Remove",
    (VALUE == "Reported") & grepl("limit", RESULT_REMARK, ignore.case = T) ~ "Impute",
    grepl("limit", RESULT_REMARK, ignore.case = T) ~ "Impute",
    grepl("Outli", RESULT_REMARK, ignore.case = T) ~ "Impute",
    grepl("<", VALUE) ~ "Impute", # Maybe just remove the less than?
    grepl("less", RESULT_REMARK, ignore.case = T) ~ "Keep",
    grepl("estimate", RESULT_REMARK, ignore.case= T) ~ "Keep",
    grepl("correction", RESULT_REMARK, ignore.case= T) ~ "Keep",
    (VALUE == "Reported")  & grepl("limit", RESULT_REMARK, ignore.case = T) ~ "Keep",
    VALUE == "Not Reported" ~ "Remove",
  )) %>%
  select(VALUE, treatment, RESULT_REMARK) %>%
  arrange(VALUE, treatment) 

naTreatment %>%
  gt() %>%
  tab_header(
    title = "Missing/estimated/Measured values",
    subtitle = "Types of reported values and their treatment"
  )
```


## Units
- For mismatched units:
  - For each Unique analyte, Fraction, and whether below 20 or not
- Saw if distributions differ
- Assume internally consistent
- Test log of distance between all pairwise combinations
- Identified mismatched units by testing if their means are significantly different
- labeled in the UNIT_DETERMINATION column

```{r unitDistributions}
unitsSheet <- df %>%
  mutate(Above = SAMPLE_DEPTH_M <20) %>%
  group_by(ANALYTE, FRACTION, METHOD, UNITS, Above) %>%
  reframe(med = median(VALUE, na.rm =T),
            METHOD = toString(METHOD)) %>% 
  arrange(ANALYTE, Above)

multUnits <- unitsSheet %>%
  count(ANALYTE, FRACTION, UNITS, Above) %>% 
  filter(n > 1) %>%
  arrange(ANALYTE, FRACTION) %>%
  select(ANALYTE, FRACTION, Above)

df %>%
  mutate(Above = SAMPLE_DEPTH_M <20) %>%
  drop_na(Above) %>%
  ggplot(aes(x = VALUE, fill = UNITS, group = UNITS)) +
    geom_histogram(alpha = 0.3, position = "identity") +
    ggh4x::facet_grid2(rows = vars(ANALYTE), cols = vars(Above), scales="free", independent = "all", remove_labels = "all", render_empty = F,
    ) +
    scale_x_log10() +
  ggtitle("Distributions of Analytes above (Right) and below (Left) 20 meters") + 
  theme_void()

```

```{r identifyUnits}

unitsSheet <- df %>%
  mutate(Above = SAMPLE_DEPTH_M <20) %>%
  group_by(ANALYTE, FRACTION, UNITS, Above) %>%
  reframe(med = median(VALUE, na.rm =T),
            METHOD = toString(METHOD)) %>% 
  arrange(ANALYTE, Above)

unitThresholds <- data.frame("lower" = 0.33,
                             "upper" = 2.5)
unitsSheet %>%
  group_by(ANALYTE, Above) %>%
  reframe(d = abs(diff(log10(med)))) %>%
  ggplot(aes(x = d)) +
  geom_histogram() +
  annotate("rect", xmin = 0.33, xmax= 2.5, ymin= -Inf, ymax = Inf, alpha= 0.2, fill = "orange", label = "fjklasdf") +
  annotate("text", x = 0.15, y = 2, label = "Same Units", size = 5, angle = 90) +
  annotate("text", x = 1, y = 2, label = "Need to Check", size = 5, angle = 90) +
  annotate("text", x = 3, y = 2, label = "Different Units", size = 5, angle = 90)
```


```{r unitMatchingness}
unitMatchingness <- unitsSheet %>%
  group_by(ANALYTE, UNITS, Above) %>% 
  reframe(d = abs(diff(log10(med)))) %>%
  mutate(UNIT_DETERMINATION = case_when(
    d > 300 ~ "Wrong Units",
    d < 2 ~ "Same Units",
    is.nan(d) ~ "Units Missing",
    .default = "Investigate further"
  ))


# df %>%
#   mutate(Above = STN_DEPTH_M < 20) %>%
#   left_join(unitMatchingness, by = c("ANALYTE", "UNITS", "Above")) %>% 
#   ggplot(aes(x  = VALUE, y= after_stat(density),
#              fill = UNIT_DETERMINATION, group = UNITS)) +
#   geom_histogram(alpha = 0.3, position = "identity") +
#   facet_wrap(~ANL_CODE, scales = "free") +
#   scale_x_log10() +
#   theme(legend.title = element_text(size = 8), legend.text = element_text(size = 8)) +
#   ggtitle("Analyte distributions ")

```

## LAT/LON precision
- Lat/Lon have varying degrees of precision. Is this from rounding?
- Missing from pre 1994
- How far is tolerable for distance from Lat/Lon center? Some are as big as 1000 meters
```{r checkLatLon}
df %>% 
  mutate(
    LATITUDE = as.character(LATITUDE),
    lat_dec = nchar(str_split_i(LATITUDE, "\\.", i =2)),
    LONGITUDE = as.character(LONGITUDE),
    lon_dec = nchar(str_split_i(LONGITUDE, "\\.", i = 2))) %>%
  select(lat_dec, lon_dec) %>%
  pivot_longer(cols = c(lat_dec, lon_dec), values_to = "decimals", names_to = "coord") %>%
  ggplot(aes(x = decimals, fill = coord)) +
  geom_bar(position = "dodge") + 
  ggtitle("Decimal places for coordinates")
```

```{r latLongPerYear}

df %>%
  group_by(YEAR) %>%
  reframe(p_missing_lon = mean(is.na(LONGITUDE))) %>%
  ggplot(aes(x = YEAR, y = p_missing_lon)) +
  geom_point(size = 5) +
  ggtitle("Coordinates missing % per year")


df %>% 
  select(STATION_ID, LATITUDE, LONGITUDE) %>%
  group_by(STATION_ID) %>%
  mutate(meanLat = mean(LATITUDE, na.rm = T), meanLon = mean(LONGITUDE, na.rm = T),
         meters = sqrt((LATITUDE - meanLat)^2 + (LONGITUDE - meanLon)^2),
         meters = meters * 111320) %>%
  ggplot(aes(x = meters)) +
  geom_histogram() + 
  scale_x_log10() +
  ggtitle("Sample distance from Station Site mean")
```

## Analyte names
- Named by hand after looking at the following table 
```{r namingTable}
#| label: namingTable
#| tbl-cap: "A list of all unique Medium, Analyte, Fraction combinations to aid in determining a unified naming system. Additionally, we've included columns for different attempts at creating a naming system"


# Read existing names, so we don't lose that work
existingNames <- readxl::read_xlsx("Analytes.xlsx", sheet=2, skip = 1) %>%
  rename(NAME_COMMENT = `...10`) %>%
  select(MEDIUM, ANL_CODE, FRACTION, `Ryan's Names`, `Kelsey's names`, NAME_COMMENT) %>%
  mutate(FRACTION = ifelse(FRACTION == "Not applicable", "", FRACTION)) 

# Make a table for naming
namingTable <- df %>%
  mutate(Above = STN_DEPTH_M < 20) %>%
  group_by(MEDIUM, ANALYTE, ANL_CODE, FRACTION, Above) %>%
  reframe(min = min(YEAR, na.rm=T), max = max(YEAR, na.rm = T),
          METHOD = toString(unique(METHOD)),
          UNITS = toString(unique(UNITS))) %>%
  left_join(unitMatchingness, by = c("ANALYTE", "UNITS", "Above")) %>%
  left_join(existingNames, by = c("MEDIUM", "ANL_CODE", "FRACTION")) %>%
  unite(Years, min, max, sep = "-", remove = T)  %>%
  arrange(MEDIUM, ANALYTE, FRACTION) %>%
  select(-c(d))  %>%
  mutate(UNIT_DETERMINATION = ifelse(is.na(UNIT_DETERMINATION), "", UNIT_DETERMINATION))

namingTable %>%
  ungroup() %>%
  gt()
```


### Compiling tables
```{r writingXcel}
writexl::write_xlsx(list("Complete Data" = df, "Unifying Names and Units" = namingTable,  "NA Treatment" = naTreatment), path = "Analytes2.xlsx")
```
